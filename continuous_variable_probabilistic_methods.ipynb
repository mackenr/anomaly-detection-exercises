{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore, kstest, shapiro\n",
    "alpha=.05\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import os\n",
    "from symtable import symtable\n",
    "\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sympy import *\n",
    "import pydataset\n",
    "import env\n",
    "import seaborn as sns\n",
    "from sympy.matrices import Matrix\n",
    "from IPython.display import display\n",
    "\n",
    "from functools import reduce\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 1. Define a function named get_lower_and_upper_bounds that has two arguments. The first argument is a pandas Series. The second argument is the multiplier, which should have a default argument of 1.5.\n",
    "\n",
    "\n",
    "\n",
    "1. Using [lemonade.csv](https://gist.githubusercontent.com/ryanorsinger/19bc7eccd6279661bd13307026628ace/raw/e4b5d6787015a4782f96cad6d1d62a8bdbac54c7/lemonade.csv) dataset and focusing on continuous variables:\n",
    "    * Use the IQR Range Rule and the upper and lower bounds to identify the lower outliers of each column of lemonade.csv, using the multiplier of 1.5. Do these lower outliers make sense? Which outliers should be kept?\n",
    "    * Use the IQR Range Rule and the upper and upper bounds to identify the upper outliers of each column of lemonade.csv, using the multiplier of 1.5. Do these upper outliers make sense? Which outliers should be kept?\n",
    "    * Using the multiplier of 3, IQR Range Rule, and the lower bounds, identify the outliers below the lower bound in each colum of lemonade.csv. Do these lower outliers make sense? Which outliers should be kept?\n",
    "    * Using the multiplier of 3, IQR Range Rule, and the upper bounds, identify the outliers above the upper_bound in each colum of lemonade.csv. Do these upper outliers make sense? Which outliers should be kept?\n",
    "2. Identify if any columns in lemonade.csv are normally distributed. For normally distributed columns:\n",
    "    * Use a 2 sigma decision rule to isolate the outliers.\n",
    "        * Do these make sense?\n",
    "        * Should certain outliers be kept or removed?\n",
    "3. Now use a 3 sigma decision rule to isolate the outliers in the normally distributed columns from lemonade.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = 'https://gist.githubusercontent.com/ryanorsinger/19bc7eccd6279661bd13307026628ace/raw/e4b5d6787015a4782f96cad6d1d62a8bdbac54c7/lemonade.csv'\n",
    "\n",
    "df = pd.read_csv(url)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# sns.relplot(x='x', y='y', col='dataset', data=df)\n",
    "# df.groupby(\"dataset\").describe()\n",
    "\n",
    "\n",
    "df['Date']=pd.to_datetime(df.Date)\n",
    "df.dtypes\n",
    "df=df.set_index('Date',drop=True)\n",
    "df.drop(columns=['Day'],inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getupperandlower(ser, k=1.5):\n",
    "    ''' remove outliers from a list of columns in a dataframe \n",
    "        and return that dataframe. This was a gift from our instructor. It is an implementation of the Tukey method.\n",
    "        https://en.wikipedia.org/wiki/Tukey%27s_range_test\n",
    "    '''\n",
    "    # Create a column that will label our rows as containing an outlier value or not\n",
    "\n",
    "\n",
    "    q1, q3 = ser.quantile([.25, .75])  # get quartiles\n",
    "\n",
    "    iqr = q3 - q1   # calculate interquartile range\n",
    "\n",
    "    upper_bound = q3 + k * iqr   # get upper bound\n",
    "    lower_bound = q1 - k * iqr   # get lower boun   \n",
    "    # update the outlier label any time that the value is outside of boundaries\n",
    "    df['outlier'] = np.where(((df[col] < lower_bound) | (df[col] > upper_bound)) & (df.outlier == False), True, df.outlier)\n",
    "    \n",
    "    # df = df[df.outlier == False]\n",
    "  \n",
    "    print(f\"Number of observations removed: {num_obs - df.shape[0]}\")\n",
    "\n",
    "\n",
    "def zscored(df,numcols,numstds):\n",
    "    zscoredDf=df[numcols].apply(zscore,axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    for col in numcols:\n",
    "    \n",
    "        # df[f'{col}_zscored']=zscoredDf[f'{col}']\n",
    "        zscoredDf[f'{col}']=zscoredDf[f'{col}'].map(lambda x: (abs(x)<=numstds))\n",
    "    \n",
    "    return df,zscoredDf\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.figure import figaspect\n",
    "from matplotlib.pyplot import axes, figtext\n",
    "\n",
    "\n",
    "def normal(df,col,alpha,printit=False):\n",
    "    s,p=shapiro(list(df[col]))\n",
    "    null=\"Our null hypothesis: 'The data is Guassian'\"\n",
    "   \n",
    "    if p >alpha:\n",
    "        normal=True\n",
    "   \n",
    "        # print(f'{null} is \\n{normal}\\np: {p} v {alpha}\\n')\n",
    "\n",
    "    else:\n",
    "        normal=False\n",
    "      \n",
    "    \n",
    "    if printit==True:\n",
    "         print(f'{col}\\n ')\n",
    "         print(f'{null} is \\n{normal}\\n')\n",
    "\n",
    "    \n",
    "    return normal\n",
    "\n",
    "\n",
    "\n",
    "def numericalNormal(df,numerical,alpha):\n",
    "    for col in numerical:\n",
    "        normal(df,col,alpha)\n",
    "    \n",
    "\n",
    "\n",
    "def univariate(df,catrange=[2,5]):\n",
    "    '''   \n",
    "    For the time being the way to select between categorical and continious is simply by declared dtypes. \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "\n",
    "    # cat,num=vartypes(df,catrange)\n",
    "    num=df.select_dtypes(include='number').columns.to_list()\n",
    "    cat=df.select_dtypes(exclude='number').columns.to_list()\n",
    "    # display(symbols('Categorical~Variables'))\n",
    "    for i in cat:\n",
    "        # display(symbols(i.replace('_','~')))\n",
    "        # display(symbols('Value~Counts'))\n",
    "        display(df[i].value_counts())\n",
    "        display(df[i].value_counts(normalize=True)*100)       \n",
    "        sns.countplot(x=i,data=df)\n",
    "        plt.show()\n",
    "    # display(symbols('Numerical~Variabless'))\n",
    "    for i in num:\n",
    "        # display(symbols(i.replace('_','~'.capitalize())))       \n",
    "        fig,axs=plt.subplots(1,3)\n",
    "        kwargs=( {'family':'sans-serif'} )\n",
    "        fig.suptitle(t=f'{i}:\\n [Hist w/ Kde] v. [Box] v. [Q-Q] ',**kwargs)\n",
    "        sns.histplot( x=i,data=df, kde=True,ax=axs[0])\n",
    "        sns.boxplot(x=i,data=df,ax=axs[1])\n",
    "        axs[2]=stats.probplot(df[i],dist='norm',plot=plt)\n",
    "        \n",
    "      \n",
    "        fig.set_figheight(8)\n",
    "        fig.set_figwidth(32)\n",
    "    \n",
    "        plt.show()\n",
    "        print(\"\\n\\n\\n\")\n",
    "\n",
    "\n",
    "def pd_DF_one_shot_info(df,log_bool=False,alpha=.05):\n",
    "    uniquevales=df.select_dtypes(include=\"number\").nunique().sort_values(ascending=False)\n",
    "    plot_u=uniquevales.plot.bar(logy=log_bool,figsize=(32,8),title='Unique values per column',rot=45)\n",
    "    total_var=df.var(axis=0)\n",
    "    madfrom=df.mad(axis=0)\n",
    "\n",
    "    total_skew=df.skew(axis=0)\n",
    "    total_kurtosis=df.kurtosis(axis=0)\n",
    "    statsuff={'var':total_var,'skew':total_skew,'kurt':total_kurtosis,'mean_abs_dev':madfrom}   \n",
    "    stats=pd.concat(statsuff,axis=1)\n",
    "    stats=stats.T\n",
    "   \n",
    "   \n",
    "    nums=df.select_dtypes(include=\"number\").columns.to_list()\n",
    "    normalornotdic={}\n",
    "    for n in nums:\n",
    "        res=normal(df,n,alpha)\n",
    "        normalornotdic.update({n:res})\n",
    "    oneshot=pd.concat(\n",
    "        [stats,\n",
    "        pd.DataFrame(df.dtypes,columns=['dtypes']).T,\n",
    "        df.describe(include='all')],axis=0)\n",
    "    oneshot=oneshot.T\n",
    "    oneshot['guassian']=''\n",
    "    oneshot['guassian']=oneshot.index.map(mapper=(lambda x: normalornotdic.get(x)))\n",
    "    oneshot=oneshot.T\n",
    "    \n",
    "    display(oneshot,plot_u)  #df.info(show_counts=True, verbose=True),\n",
    " \n",
    "   \n",
    "  \n",
    "    univariate(df)\n",
    "\n",
    "    # oneshot=oneshot.merge(right=normalornotdic,left_index=True,how='cross')\n",
    "        \n",
    "\n",
    "# Plot information with y-axis in log-scale\n",
    "        \n",
    "   \n",
    "    return oneshot\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "oneshot=pd_DF_one_shot_info(df,log_bool=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "cols=df.columns.to_list()\n",
    "\n",
    "df,zcored=zscored(df,cols,1)\n",
    "display(df,zcored)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def univariate(df,catrange=[2,5]):\n",
    "\n",
    "    # cat,num=vartypes(df,catrange)\n",
    "    num=df.select_dtypes(include='number').columns.to_list()\n",
    "    cat=df.select_dtypes(exlude='number').columns.to_list()\n",
    "    display(symbols('Categorical~Variables'))\n",
    "    for i in cat:\n",
    "        display(symbols(i.replace('_','~')))\n",
    "        display(symbols('Value~Counts'))\n",
    "        display(df[i].value_counts())\n",
    "        display(df[i].value_counts(normalize=True)*100)       \n",
    "        sns.countplot(x=i,data=df)\n",
    "        plt.show()\n",
    "    display(symbols('Numerical~Variabless'))\n",
    "    for i in num:\n",
    "        display(symbols(i.replace('_','~'.capitalize())))       \n",
    "        sns.boxplot(x=i,data=df)\n",
    "        plt.show()\n",
    "        df[i].hist()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### https://matplotlib.org/stable/gallery/statistics/confidence_ellipse.html\n",
    "\n",
    "\n",
    "from matplotlib.patches import Ellipse\n",
    "import matplotlib.transforms as transforms\n",
    "\n",
    "\n",
    "def confidence_ellipse(x, y, ax, n_std=3.0, facecolor='none', **kwargs):\n",
    "    \"\"\"\n",
    "    Create a plot of the covariance confidence ellipse of *x* and *y*.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y : array-like, shape (n, )\n",
    "        Input data.\n",
    "\n",
    "    ax : matplotlib.axes.Axes\n",
    "        The axes object to draw the ellipse into.\n",
    "\n",
    "    n_std : float\n",
    "        The number of standard deviations to determine the ellipse's radiuses.\n",
    "\n",
    "    **kwargs\n",
    "        Forwarded to `~matplotlib.patches.Ellipse`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.patches.Ellipse\n",
    "    \"\"\"\n",
    "    if x.size != y.size:\n",
    "        raise ValueError(\"x and y must be the same size\")\n",
    "\n",
    "    cov = np.cov(x, y)\n",
    "    pearson = cov[0, 1]/np.sqrt(cov[0, 0] * cov[1, 1])\n",
    "    # Using a special case to obtain the eigenvalues of this\n",
    "    # two-dimensional dataset.\n",
    "    ell_radius_x = np.sqrt(1 + pearson)\n",
    "    ell_radius_y = np.sqrt(1 - pearson)\n",
    "    ellipse = Ellipse((0, 0), width=ell_radius_x * 2, height=ell_radius_y * 2,\n",
    "                      facecolor=facecolor, **kwargs)\n",
    "\n",
    "    # Calculating the standard deviation of x from\n",
    "    # the squareroot of the variance and multiplying\n",
    "    # with the given number of standard deviations.\n",
    "    scale_x = np.sqrt(cov[0, 0]) * n_std\n",
    "    mean_x = np.mean(x)\n",
    "\n",
    "    # calculating the standard deviation of y ...\n",
    "    scale_y = np.sqrt(cov[1, 1]) * n_std\n",
    "    mean_y = np.mean(y)\n",
    "\n",
    "    transf = transforms.Affine2D() \\\n",
    "        .rotate_deg(45) \\\n",
    "        .scale(scale_x, scale_y) \\\n",
    "        .translate(mean_x, mean_y)\n",
    "\n",
    "    ellipse.set_transform(transf + ax.transData)\n",
    "    return ax.add_patch(ellipse)\n",
    "\n",
    "\n",
    "###plot\n",
    "\n",
    "\n",
    "def get_correlated_dataset(n, dependency, mu, scale):\n",
    "    latent = np.random.randn(n, 2)\n",
    "    dependent = latent.dot(dependency)\n",
    "    scaled = dependent * scale\n",
    "    scaled_with_offset = scaled + mu\n",
    "    # return x and y of the new, correlated dataset\n",
    "    return scaled_with_offset[:, 0], scaled_with_offset[:, 1]\n",
    "\n",
    "fig, ax_nstd = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "dependency_nstd = [[0.8, 0.75],\n",
    "                   [-0.2, 0.35]]\n",
    "mu = 0, 0\n",
    "scale = 8, 5\n",
    "\n",
    "ax_nstd.axvline(c='grey', lw=1)\n",
    "ax_nstd.axhline(c='grey', lw=1)\n",
    "\n",
    "x, y = get_correlated_dataset(500, dependency_nstd, mu, scale)\n",
    "ax_nstd.scatter(x, y, s=0.5)\n",
    "\n",
    "confidence_ellipse(x, y, ax_nstd, n_std=1,\n",
    "                   label=r'$1\\sigma$', edgecolor='firebrick')\n",
    "confidence_ellipse(x, y, ax_nstd, n_std=2,\n",
    "                   label=r'$2\\sigma$', edgecolor='fuchsia', linestyle='--')\n",
    "confidence_ellipse(x, y, ax_nstd, n_std=3,\n",
    "                   label=r'$3\\sigma$', edgecolor='blue', linestyle=':')\n",
    "\n",
    "ax_nstd.scatter(mu[0], mu[1], c='red', s=3)\n",
    "ax_nstd.set_title('Different standard deviations')\n",
    "ax_nstd.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "PARAMETERS = {\n",
    "    'Positive correlation': [[0.85, 0.35],\n",
    "                             [0.15, -0.65]],\n",
    "    'Negative correlation': [[0.9, -0.4],\n",
    "                             [0.1, -0.6]],\n",
    "    'Weak correlation': [[1, 0],\n",
    "                         [0, 1]],\n",
    "}\n",
    "\n",
    "mu = 2, 4\n",
    "scale = 3, 5\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(9, 3))\n",
    "for ax, (title, dependency) in zip(axs, PARAMETERS.items()):\n",
    "    x, y = get_correlated_dataset(800, dependency, mu, scale)\n",
    "    ax.scatter(x, y, s=0.5)\n",
    "\n",
    "    ax.axvline(c='grey', lw=1)\n",
    "    ax.axhline(c='grey', lw=1)\n",
    "\n",
    "    confidence_ellipse(x, y, ax, edgecolor='red')\n",
    "\n",
    "    ax.scatter(mu[0], mu[1], c='red', s=3)\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for some series s:\n",
    "def get_upper_outliers(s, k=1.5):\n",
    "    q1, q3 = s.quantile([0.25, 0.75])\n",
    "    iqr = q3 - q1\n",
    "    upper_bound = q3 + k*iqr\n",
    "    return s.apply(lambda x: max([x - upper_bound, 0]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def add_upper_outlier_columns(df, k=1.5):\n",
    "    for col in df.select_dtypes('number'):\n",
    "        df[col + '_upper_outliers'] = get_upper_outliers(df[col], k)\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_columns(df, cols_to_remove):\n",
    "    df = df.drop(columns=cols_to_remove)\n",
    "    return df\n",
    "\n",
    "def handle_missing_values(df, \n",
    "                          prop_required_columns=0.5, \n",
    "                          prop_required_row=0.75):\n",
    "    threshold = int(round(prop_required_columns * len(df.index), 0))\n",
    "    df = df.dropna(axis=1, thresh=threshold)\n",
    "    threshold = int(round(prop_required_row * len(df.columns), 0))\n",
    "    df = df.dropna(axis=0, thresh=threshold)\n",
    "    return df  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
